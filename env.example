# Optional
# Hugging Face access token with read access to model weights.
# Required only for gated/private models.
HF_TOKEN=hf_put_your_token_here

# This variable is only used with Dockerfile.gpu / docker-compose.gpu.yaml
# Select a PyTorch GPU base tag from https://hub.docker.com/r/pytorch/pytorch/tags
# Use the tag only; Dockerfile.gpu resolves to "pytorch/pytorch:TAG".
# Example: 2.10.0-cuda13.0-cudnn9-runtime
PYTORCH_TAG=2.10.0-cuda13.0-cudnn9-runtime
# Optional: install flash-attn in the GPU image (0=disable, 1=enable)
INSTALL_FLASH_ATTN=0

# Device selection: auto (default), cpu, or cuda.
EMBEDDINGS_DEVICE=auto
# Optional CUDA memory cap per process (fraction of total GPU memory).
# Example: 0.5 uses up to 50% of the GPU's memory.
EMBEDDINGS_CUDA_MEMORY_FRACTION=0.5
# Optional: enable bitsandbytes quantization (requires bitsandbytes installed).
# Allowed values: 8bit, 4bit. Leave empty to disable.
EMBEDDINGS_BITSANDBYTES=

# Optional: comma/space-separated API keys for Bearer auth on /v1/embeddings.
# Leave empty to disable authentication.
EMBEDDINGS_API_KEYS=

# Max number of model bundles cached in memory.
EMBEDDINGS_MAX_LOADED_MODELS=1

# Service
# Bind host for Uvicorn inside the container/local run.
EMBEDDINGS_HOST=0.0.0.0
# Port for the API (also used for Docker port mapping).
EMBEDDINGS_PORT=11445

# Request limits
# Max texts per request batch.
EMBEDDINGS_MAX_BATCH_SIZE=2048
# Max tokens per input (after tokenization).
EMBEDDINGS_MAX_INPUT_TOKENS=8192
# Max total tokens per request (sum across inputs).
EMBEDDINGS_MAX_TOTAL_TOKENS=300000
# Max request body size in bytes.
EMBEDDINGS_MAX_REQUEST_BYTES=2000000
# Max concurrent inference calls per process.
EMBEDDINGS_MAX_CONCURRENT_INFERENCE=2
# Inference timeout in seconds.
EMBEDDINGS_INFERENCE_TIMEOUT_SECONDS=60
# Whether to truncate inputs that exceed max token limit.
EMBEDDINGS_TRUNCATE_INPUTS=false
